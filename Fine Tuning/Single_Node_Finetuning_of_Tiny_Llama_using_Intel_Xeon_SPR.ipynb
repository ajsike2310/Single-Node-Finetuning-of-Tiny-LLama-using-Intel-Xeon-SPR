{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Single Node Finetuning of Tiny LLama using Intel Xeon SPR**"
      ],
      "metadata": {
        "id": "kApmUWiQCH0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Environment"
      ],
      "metadata": {
        "id": "yqjod49ACTsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following line by line in the terminal"
      ],
      "metadata": {
        "id": "4i2rFvEaEmja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conda create -n itrex-1 python=3.10 -y\n",
        "conda activate itrex-1\n",
        "pip install intel-extension-for-transformers\n",
        "git clone https://github.com/eternalflame02/Single-Node-FInetuning-of-Tiny-LLama-using-Intel-Xeon-SPR.git\n",
        "cd ./Single-Node-FInetuning-of-Tiny-LLama-using-Intel-Xeon-SPR/Fine Tuning/\n",
        "pip install -r requirements.txt\n",
        "huggingface-cli login\n",
        "python3 -m pip install jupyter ipykernel\n",
        "python3 -m ipykernel install --name neural-chat--user"
      ],
      "metadata": {
        "id": "bNPoV857CPFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a token in https://huggingface.co/settings/tokens insert them in the huggingface longin interface."
      ],
      "metadata": {
        "id": "P1DFMFJGHgBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run rest of the cell."
      ],
      "metadata": {
        "id": "TcqZz2cuIP9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./Single-Node-FInetuning-of-Tiny-LLama-using-Intel-Xeon-SPR/Fine Tuning/\n",
        "!pip install -r requirements.txt\n",
        "%cd ../../../"
      ],
      "metadata": {
        "id": "RSlrbXkTHTTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Dataset\n",
        "\n",
        "Text Generation (General domain instruction): We use the Alpaca dataset(https://github.com/tatsu-lab/stanford_alpaca) from Stanford University as the general domain dataset to fine-tune the model. This dataset is provided in the form of a JSON file, alpaca_data.json. In Alpaca, researchers have manually crafted 175 seed tasks to guide text-davinci-003 in generating 52K instruction data for diverse tasks."
      ],
      "metadata": {
        "id": "0ge1eJG6IiN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json"
      ],
      "metadata": {
        "id": "pf0MnobxIhpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Your Chatbot\n",
        "We employ the LoRA approach to finetune the LLM efficiently."
      ],
      "metadata": {
        "id": "yxuEb1vAJgIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune the TinyLlama on Alpaca-format dataset to conduct text generation:"
      ],
      "metadata": {
        "id": "gVik9Ar6JoLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from transformers import TrainingArguments\n",
        "from intel_extension_for_transformers.neural_chat.config import (\n",
        "    ModelArguments,\n",
        "    DataArguments,\n",
        "    FinetuningArguments,\n",
        "    TextGenerationFinetuningConfig,\n",
        ")\n",
        "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
        "\n",
        "import os\n",
        "\n",
        "# Define model arguments\n",
        "model_args = ModelArguments(model_name_or_path=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Define data arguments\n",
        "data_args = DataArguments(train_file=\"alpaca_data.json\", validation_split_percentage=1)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='tinyllama',\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=4e-5,  # Adjust learning rate\n",
        "    weight_decay=0.01,  # Add weight decay\n",
        "    num_train_epochs=1,  # Increase epochs for better training\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    seed=42,  # Set random seed\n",
        "    bf16=True,\n",
        "    '''resume_from_checkpoint=True'''  # Add this to load from latest checkpoint incase the finetuning fails after creation of a checkpoint.\n",
        ")\n",
        "\n",
        "# Define finetuning arguments\n",
        "finetune_args = FinetuningArguments()\n",
        "\n",
        "# Create finetuning configuration\n",
        "finetune_cfg = TextGenerationFinetuningConfig(\n",
        "    model_args=model_args,\n",
        "    data_args=data_args,\n",
        "    training_args=training_args,\n",
        "    finetune_args=finetune_args,\n",
        ")\n",
        "\n",
        "# Start the finetuning process\n",
        "finetune_model(finetune_cfg)"
      ],
      "metadata": {
        "id": "iQEDc83VJnd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Finetuning create a duplicate of adapter_config.json and rename it to config.json"
      ],
      "metadata": {
        "id": "1K_J91_yMs4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying Chatbot"
      ],
      "metadata": {
        "id": "DiAHkU8DMfhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customize chatbot with the new llm model."
      ],
      "metadata": {
        "id": "NH-ZPTNWMLkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from intel_extension_for_transformers.neural_chat import build_chatbot  # Import function to build a chatbot\n",
        "from intel_extension_for_transformers.neural_chat import PipelineConfig  # Import configuration for the chatbot pipeline\n",
        "from intel_extension_for_transformers.neural_chat.config import LoadingModelConfig  # Import configuration for loading the model\n",
        "\n",
        "# Create a pipeline configuration specifying the model and loading options\n",
        "config = PipelineConfig(\n",
        "    model_name_or_path=\"./tinyllama\",\n",
        "    loading_config=LoadingModelConfig(\n",
        "        peft_path=\"./tinyllama\"  # Path to the PEFT model (fine-tuned model)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build a chatbot instance using the defined configuration\n",
        "chatbot = build_chatbot(config)\n",
        "\n",
        "# Generate a response to the user query\n",
        "query = \"Tell me about AI.\"\n",
        "response = chatbot.predict(query=query)\n",
        "\n",
        "# Print the generated response\n",
        "print(response)"
      ],
      "metadata": {
        "id": "jSf176JsKy-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}